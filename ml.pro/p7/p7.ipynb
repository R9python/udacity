{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习进阶（S13）毕业项目 - 猫狗大战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 项目：识别图片中是猫还是狗\n",
    "---\n",
    "* 此项目是kaggle平台上的一个比赛项目，最终的要求是提供一个模型来识别图片中的对象是猫还是狗。\n",
    "* 这里会使用深度学习中的卷积神经网络来构建模型。\n",
    "* 最总要求测试评分进入 kaggle Public Leaderboard 前10%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 1: 数据探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "                      \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from keras.applications.inception_resnet_v2 import *\n",
    "from keras.applications.xception import *\n",
    "from keras.applications.inception_v3 import *\n",
    "\n",
    "from operator import is_not\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog.9220.jpg', 'dog.4153.jpg', 'cat.4113.jpg', 'dog.7547.jpg', 'cat.7459.jpg', 'dog.10028.jpg', 'cat.11954.jpg', 'cat.7368.jpg', 'dog.3505.jpg', 'cat.12057.jpg']\n"
     ]
    }
   ],
   "source": [
    "# 读取训练集\n",
    "train_files = os.listdir('train')\n",
    "print(train_files[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 寻找异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义ImageNet中识别猫狗的分类 https://blog.csdn.net/zhangjunbob/article/details/53258524\n",
    "dogs = [\n",
    " 'n02085620','n02085782','n02085936','n02086079','n02086240','n02086646','n02086910','n02087046'\n",
    ",'n02087394','n02088094','n02088238','n02088364','n02088466','n02088632','n02089078','n02089867'\n",
    ",'n02089973','n02090379','n02090622','n02090721','n02091032','n02091134','n02091244','n02091467'\n",
    ",'n02091635','n02091831','n02092002','n02092339','n02093256','n02093428','n02093647','n02093754'\n",
    ",'n02093859','n02093991','n02094114','n02094258','n02094433','n02095314','n02095570','n02095889'\n",
    ",'n02096051','n02096177','n02096294','n02096437','n02096585','n02097047','n02097130','n02097209'\n",
    ",'n02097298','n02097474','n02097658','n02098105','n02098286','n02098413','n02099267','n02099429'\n",
    ",'n02099601','n02099712','n02099849','n02100236','n02100583','n02100735','n02100877','n02101006'\n",
    ",'n02101388','n02101556','n02102040','n02102177','n02102318','n02102480','n02102973','n02104029'\n",
    ",'n02104365','n02105056','n02105162','n02105251','n02105412','n02105505','n02105641','n02105855'\n",
    ",'n02106030','n02106166','n02106382','n02106550','n02106662','n02107142','n02107312','n02107574'\n",
    ",'n02107683','n02107908','n02108000','n02108089','n02108422','n02108551','n02108915','n02109047'\n",
    ",'n02109525','n02109961','n02110063','n02110185','n02110341','n02110627','n02110806','n02110958'\n",
    ",'n02111129','n02111277','n02111500','n02111889','n02112018','n02112137','n02112350','n02112706'\n",
    ",'n02113023','n02113186','n02113624','n02113712','n02113799','n02113978']\n",
    "\n",
    "cats=['n02123045','n02123159','n02123394','n02123597','n02124075','n02125311','n02127052']\n",
    "\n",
    "\n",
    "\n",
    "#存储不能识别为猫狗的异常图片\n",
    "my_unknow_images = []\n",
    "\n",
    "\n",
    "# 根据模型预测的结果查看是否属于猫狗的范围\n",
    "def is_cat_dog(preds_result):\n",
    "    my_labels = dogs[:]\n",
    "    my_labels[len(my_labels):len(my_labels)] = cats\n",
    "    a = [x[0] in my_labels for x in preds_result]\n",
    "#     print(sum(a))\n",
    "    return sum(a) > 0\n",
    "\n",
    "# iscatdog = is_cat_dog([(u'n02504013', u'Indian_elephant', 0.82658225), \n",
    "#                        (u'n01871265', u'tusker', 0.1122357), \n",
    "#                        (u'n02123394', u'test', 0.0000000)])\n",
    "# print(iscatdog)\n",
    "\n",
    "# # 输入：模型, 图片路径, 图片resize尺寸, top number, 预处理function\n",
    "# # 输出：预测概率，格式(class, description, probability)\n",
    "# #       例如 [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357)]\n",
    "def my_predict(model, img_path_list, resize, top_n, preprocess_type):\n",
    "    print(preprocess_type + \" processing...\")\n",
    "    for image_path in tqdm(img_path_list):\n",
    "        img = image.load_img('train/'+image_path, target_size=resize)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        if preprocess_type == 'InceptionResNetV2':\n",
    "            x = inception_resnet_v2.preprocess_input(x)\n",
    "        elif preprocess_type == 'Xception':\n",
    "            x = xception.preprocess_input(x)\n",
    "        elif preprocess_type == 'InceptionV3':\n",
    "            x = inception_v3.preprocess_input(x)\n",
    "\n",
    "        preds = model.predict(x)\n",
    "        result = decode_predictions(preds, top=top_n)[0]\n",
    "        if not is_cat_dog(result):\n",
    "            my_unknow_images.append(image_path)\n",
    "\n",
    "\n",
    "# 采用InceptionResNetV2预测\n",
    "# 参考 https://keras.io/zh/applications/#applications\n",
    "# irv2(1000 samples): top3 -> 42 pics, top10 -> 13 pics, top20 -> 6 pics\n",
    "# model_irv2 = InceptionResNetV2(weights='imagenet')\n",
    "# my_predict(model_irv2,train_files,(299, 299),20, 'InceptionResNetV2')\n",
    "\n",
    "# 采用Xception预测\n",
    "# model_xception = Xception(weights='imagenet')\n",
    "# my_predict(model_xception,train_files,(299, 299),20, 'Xception')\n",
    "\n",
    "# 采用InceptionV3预测\n",
    "# model_inceptionv3 = InceptionV3(weights='imagenet')\n",
    "# my_predict(model_inceptionv3,train_files,(224, 224),20, 'InceptionV3')\n",
    "\n",
    "# print(my_unknow_images)\n",
    "# print(len(my_unknow_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去重后, 图片共 148\n"
     ]
    }
   ],
   "source": [
    "# 通过取三个模型预测的非猫非狗的数据集之和\n",
    "my_unknow_images = ['cat.12424.jpg', 'dog.11266.jpg', 'cat.1928.jpg', 'dog.1194.jpg', 'cat.11661.jpg', 'cat.7372.jpg', 'dog.1043.jpg', 'cat.6754.jpg', 'cat.767.jpg', 'cat.10365.jpg', 'cat.7964.jpg', 'cat.12182.jpg', 'dog.3889.jpg', 'cat.10176.jpg', 'cat.714.jpg', 'cat.9171.jpg', 'cat.11386.jpg', 'dog.6725.jpg', 'cat.6345.jpg', 'cat.5418.jpg', 'cat.4688.jpg', 'cat.5816.jpg', 'cat.7564.jpg', 'cat.12272.jpg', 'cat.12073.jpg', 'cat.2939.jpg', 'cat.4906.jpg', 'cat.5351.jpg', 'cat.7377.jpg', 'cat.2496.jpg', 'cat.4503.jpg', 'cat.10084.jpg', 'dog.9517.jpg', 'dog.10237.jpg', 'cat.12380.jpg', 'cat.10276.jpg', 'dog.9188.jpg', 'cat.9377.jpg', 'cat.4353.jpg', 'cat.4355.jpg', 'dog.6475.jpg', 'cat.7175.jpg', 'dog.5604.jpg', 'dog.10161.jpg', 'cat.4308.jpg', 'cat.10945.jpg', 'cat.8470.jpg', 'cat.5456.jpg', 'cat.388.jpg', 'cat.3868.jpg', 'dog.1895.jpg', 'dog.10801.jpg', 'cat.10700.jpg', 'cat.7682.jpg', 'cat.11777.jpg', 'cat.10029.jpg', 'dog.1773.jpg', 'cat.10151.jpg', 'cat.7728.jpg', 'cat.124.jpg', 'cat.6573.jpg', 'cat.2433.jpg', 'cat.5071.jpg', 'cat.3658.jpg', 'cat.8456.jpg', 'dog.11299.jpg', 'cat.9163.jpg', 'dog.12155.jpg', 'cat.6451.jpg', 'dog.2614.jpg', 'dog.1308.jpg', 'cat.5843.jpg', 'cat.7968.jpg', 'dog.1259.jpg', 'cat.11159.jpg', 'cat.3004.jpg', 'dog.10747.jpg', 'cat.4824.jpg', 'dog.2422.jpg', 'cat.10712.jpg', 'cat.7464.jpg', 'cat.1747.jpg', 'cat.9731.jpg', 'dog.4367.jpg', 'dog.12376.jpg', 'cat.2598.jpg', 'cat.7976.jpg', 'dog.8736.jpg', 'cat.11256.jpg', 'dog.10190.jpg', 'cat.4198.jpg', 'cat.1119.jpg', 'cat.9360.jpg', 'dog.2578.jpg', 'cat.4338.jpg', 'cat.10634.jpg', 'cat.2520.jpg', 'cat.3216.jpg', 'cat.3672.jpg', 'cat.11969.jpg', 'cat.12424.jpg', 'cat.6590.jpg', 'dog.1194.jpg', 'cat.10365.jpg', 'dog.3889.jpg', 'cat.10176.jpg', 'cat.12476.jpg', 'cat.9171.jpg', 'cat.2337.jpg', 'dog.6725.jpg', 'cat.5418.jpg', 'cat.4688.jpg', 'cat.11607.jpg', 'cat.7564.jpg', 'cat.12272.jpg', 'cat.2939.jpg', 'cat.5351.jpg', 'cat.7377.jpg', 'cat.8921.jpg', 'dog.9517.jpg', 'dog.10237.jpg', 'dog.9188.jpg', 'cat.519.jpg', 'cat.11237.jpg', 'dog.6475.jpg', 'dog.5604.jpg', 'dog.10161.jpg', 'cat.4308.jpg', 'cat.8470.jpg', 'cat.10636.jpg', 'dog.1895.jpg', 'dog.10801.jpg', 'cat.11184.jpg', 'cat.7682.jpg', 'cat.5804.jpg', 'cat.10029.jpg', 'dog.1773.jpg', 'cat.2433.jpg', 'cat.5071.jpg', 'cat.8456.jpg', 'dog.11299.jpg', 'cat.8286.jpg', 'dog.12148.jpg', 'dog.12155.jpg', 'cat.5609.jpg', 'dog.2614.jpg', 'cat.9494.jpg', 'dog.1308.jpg', 'cat.5492.jpg', 'cat.7968.jpg', 'dog.1259.jpg', 'dog.10747.jpg', 'dog.2422.jpg', 'cat.10712.jpg', 'cat.4833.jpg', 'cat.2429.jpg', 'dog.8898.jpg', 'dog.4367.jpg', 'dog.12376.jpg', 'dog.8736.jpg', 'dog.10190.jpg', 'cat.6696.jpg', 'cat.4338.jpg', 'cat.3123.jpg', 'cat.2520.jpg', 'cat.3216.jpg', 'cat.3672.jpg', 'dog.11266.jpg', 'dog.1194.jpg', 'cat.7899.jpg', 'cat.9983.jpg', 'dog.1043.jpg', 'dog.3889.jpg', 'cat.12493.jpg', 'cat.9171.jpg', 'cat.2337.jpg', 'dog.6725.jpg', 'dog.10939.jpg', 'cat.5418.jpg', 'cat.4688.jpg', 'cat.11607.jpg', 'cat.7564.jpg', 'cat.12272.jpg', 'cat.2939.jpg', 'cat.8383.jpg', 'cat.8029.jpg', 'cat.5324.jpg', 'cat.5351.jpg', 'cat.7377.jpg', 'dog.10842.jpg', 'cat.8921.jpg', 'dog.9517.jpg', 'dog.10237.jpg', 'cat.4514.jpg', 'dog.9931.jpg', 'dog.9188.jpg', 'cat.334.jpg', 'dog.6475.jpg', 'dog.5604.jpg', 'dog.10161.jpg', 'cat.8470.jpg', 'cat.10636.jpg', 'cat.3868.jpg', 'dog.1895.jpg', 'cat.8118.jpg', 'cat.4549.jpg', 'dog.10801.jpg', 'cat.4786.jpg', 'cat.10700.jpg', 'cat.11184.jpg', 'cat.10029.jpg', 'dog.1773.jpg', 'cat.6573.jpg', 'cat.11004.jpg', 'cat.8456.jpg', 'cat.6451.jpg', 'cat.426.jpg', 'dog.2614.jpg', 'cat.10539.jpg', 'cat.5974.jpg', 'cat.7968.jpg', 'cat.9520.jpg', 'cat.6402.jpg', 'cat.12336.jpg', 'dog.4507.jpg', 'cat.3004.jpg', 'dog.10747.jpg', 'dog.2422.jpg', 'cat.10712.jpg', 'cat.4833.jpg', 'dog.4367.jpg', 'dog.4218.jpg', 'cat.7111.jpg', 'dog.12376.jpg', 'cat.11683.jpg', 'cat.10188.jpg', 'dog.8736.jpg', 'dog.10190.jpg', 'dog.7727.jpg', 'cat.4338.jpg', 'cat.3216.jpg', 'cat.10893.jpg']\n",
    "#去重后得到148张图片\n",
    "my_unknow_images = list(set(my_unknow_images))\n",
    "print('去重后, 图片共', len(my_unknow_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 显示异常图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "异常图片共 40\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train/cat.3672.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fdf86538ce55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# 显示最终获得的异常图片\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_unknow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-fdf86538ce55>\u001b[0m in \u001b[0;36mshow_image\u001b[0;34m(img_path_list)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mimgplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpilread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m             raise ValueError('Only know how to handle extensions: %s; '\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mpilread\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train/cat.3672.jpg'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "plt.figure(figsize=(16, 16))\n",
    "def show_image(img_path_list):\n",
    "    for i, image_path in  enumerate(img_path_list):\n",
    "#         print(i)\n",
    "#         print(image_path)\n",
    "        plt.subplot(10, 4, i+1)\n",
    "        \n",
    "        plt.title(image_path)\n",
    "        plt.axis('off')\n",
    "        img = mpimg.imread('train/' + image_path)\n",
    "        imgplot = plt.imshow(img)\n",
    "\n",
    "\n",
    "total_batch = math.ceil(len(my_unknow_images) / 40)\n",
    "# print(total_batch)\n",
    "\n",
    "# 分批人工审查\n",
    "# batch=3 # 0 to 3\n",
    "# show_image(my_unknow_images[batch*40:(batch+1)*40])\n",
    "\n",
    "batch0_unknow = ['cat.3672.jpg','dog.9517.jpg','cat.5418.jpg','dog.6475.jpg','dog.10801.jpg','dog.5604.jpg','cat.6402.jpg']\n",
    "batch1_unknow = ['dog.4367.jpg','dog.11299.jpg','cat.10712.jpg','cat.4338.jpg','cat.11184.jpg','dog.12376.jpg','cat.9171.jpg','cat.2337.jpg','cat.7377.jpg','dog.7727.jpg','cat.8456.jpg','dog.10161.jpg']\n",
    "batch2_unknow = ['cat.5351.jpg','dog.3889.jpg','dog.1773.jpg','dog.1194.jpg','dog.1043.jpg','dog.9931.jpg','cat.4688.jpg','dog.8898.jpg','dog.6725.jpg','cat.12493.jpg','cat.10029.jpg']\n",
    "batch3_unknow = ['dog.4507.jpg','dog.10237.jpg','dog.10747.jpg','cat.7564.jpg','cat.2520.jpg','cat.3216.jpg','cat.12272.jpg','dog.8736.jpg','cat.10700.jpg','cat.10539.jpg']\n",
    "\n",
    "# 合并分批审核结果\n",
    "final_unknow = batch0_unknow + batch1_unknow + batch2_unknow + batch3_unknow\n",
    "print('异常图片共', len(final_unknow))\n",
    "\n",
    "# 显示最终获得的异常图片\n",
    "show_image(final_unknow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理异常图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把异常图片从train移动到unknown文件夹\n",
    "def move_images_to(files, dest_path):\n",
    "    if not os.path.exists(dest_path):\n",
    "        os.mkdir(dest_path)\n",
    "        \n",
    "    for file in files:\n",
    "        shutil.move(\"train/\" + file, dest_path)\n",
    "    \n",
    "# files = ['cat.3.jpg','cat.5.jpg']\n",
    "move_images_to(final_unknow, 'unknow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 准备训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my train data count: 19968 ; my valid data count: 4992\n",
      "train cat count: 9955 ; train dog count: 10013\n",
      "valid cat count: 2525 ; valid dog count: 2467\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_files = os.listdir('train')\n",
    "test_files = os.listdir('test')\n",
    "\n",
    "# 参考 http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "my_train_files, my_valid_files = train_test_split(train_files,test_size=0.2,random_state = 0)\n",
    "print('my train data count:', len(my_train_files), '; my valid data count:', len(my_valid_files))\n",
    "\n",
    "# my train data 细分为cat和dog\n",
    "train_cat_list = [file if file.split('.')[0] == 'cat' else None for file in my_train_files]\n",
    "train_cat_list = list(filter(None.__ne__, train_cat_list))\n",
    "# train_cat_list = filter(partial(is_not, None), train_cat_list)\n",
    "\n",
    "train_dog_list = [file if file.split('.')[0] == 'dog' else None for file in my_train_files]\n",
    "train_dog_list = list(filter(None.__ne__, train_dog_list))\n",
    "# train_dog_list = filter(partial(is_not, None), train_dog_list)\n",
    "\n",
    "print('train cat count:', len(train_cat_list), '; train dog count:', len(train_dog_list))\n",
    "\n",
    "# print(train_cat_list[0:10])\n",
    "# print(train_dog_list[0:10])\n",
    "\n",
    "# my valid data 细分为cat， dog\n",
    "valid_cat_list = [file if file.split('.')[0] == 'cat' else None for file in my_valid_files]\n",
    "valid_cat_list = list(filter(None.__ne__, valid_cat_list))\n",
    "\n",
    "valid_dog_list = [file if file.split('.')[0] == 'dog' else None for file in my_valid_files]\n",
    "valid_dog_list = list(filter(None.__ne__, valid_dog_list))\n",
    "\n",
    "print('valid cat count:', len(valid_cat_list),'; valid dog count:', len(valid_dog_list))\n",
    "\n",
    "# print(valid_cat_list[0:10])\n",
    "# print(valid_dog_list[0:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文件目录分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my train data count: 9955 ; my valid data count: 2525\n"
     ]
    }
   ],
   "source": [
    "def my_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "\n",
    "my_mkdir('train_classify')\n",
    "my_mkdir('train_classify/cat')\n",
    "my_mkdir('train_classify/dog')\n",
    "my_mkdir('valid_classify')\n",
    "my_mkdir('valid_classify/cat')\n",
    "my_mkdir('valid_classify/dog')\n",
    "my_mkdir('test_nolabel')\n",
    "\n",
    "\n",
    "for file in train_cat_list:\n",
    "    os.symlink('../../train/' + file, 'train_classify/cat/' + file)\n",
    "    \n",
    "for file in train_dog_list:\n",
    "    os.symlink('../../train/' + file, 'train_classify/dog/' + file)\n",
    "\n",
    "for file in valid_cat_list:\n",
    "    os.symlink('../../train/' + file, 'valid_classify/cat/' + file)\n",
    "\n",
    "for file in valid_cat_list:\n",
    "    os.symlink('../../train/' + file, 'valid_classify/dog/' + file)\n",
    "    \n",
    "for file in test_files:\n",
    "    os.symlink('../test/' + file, 'test_nolabel/' + file)\n",
    "\n",
    "# 对测试集预处理\n",
    "def my_generator(resize,preprocess_type):\n",
    "    X_test = []\n",
    "    for file in tqdm(test_files):\n",
    "        img = image.load_img('test_nolabel/'+file, target_size=resize)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "#         x = preprocess_input(x)\n",
    "        if preprocess_type == 'InceptionResNetV2':\n",
    "            x = inception_resnet_v2.preprocess_input(x)\n",
    "        elif preprocess_type == 'Xception':\n",
    "            x = xception.preprocess_input(x)\n",
    "        elif preprocess_type == 'InceptionV3':\n",
    "            x = inception_v3.preprocess_input(x)\n",
    "        elif preprocess_type == 'ResNet50':\n",
    "            x = resnet50.preprocess_input(x)\n",
    "        X_test.append(x)\n",
    "\n",
    "\n",
    "    gen = ImageDataGenerator() # 参考 https://keras.io/zh/preprocessing/image/#imagedatagenerator\n",
    "    train_generator = gen.flow_from_directory(directory=\"train_classify\", \n",
    "                                              target_size=resize, \n",
    "                                              shuffle=False, \n",
    "                                              batch_size=32,\n",
    "                                              class_mode=\"categorical\")\n",
    "\n",
    "    valid_generator = gen.flow_from_directory(directory='valid_classify',\n",
    "                                                   target_size=resize,\n",
    "                                                   shuffle=False, \n",
    "                                                   batch_size=32,\n",
    "                                                   class_mode='categorical')\n",
    "\n",
    "    test_generator = gen.flow_from_directory(directory=\"test_nolabel\",\n",
    "                                             target_size=resize, \n",
    "                                             shuffle=False, \n",
    "                                             batch_size=32, \n",
    "                                             class_mode=None)\n",
    "    return X_test, train_generator, valid_generator, test_generator\n",
    "\n",
    "# X_test, train_generator, valid_generator, test_generator = my_generator((299, 299),'InceptionResNetV2')\n",
    "# X_test, train_generator, valid_generator, test_generator = my_generator((299, 299),'InceptionV3')\n",
    "# X_test, train_generator, valid_generator, test_generator = my_generator((224, 224),'ResNet50')\n",
    "\n",
    "train_classify_files = os.listdir('train_classify/cat')\n",
    "valid_classify_files = os.listdir('valid_classify/cat')\n",
    "print('my train data count:', len(train_classify_files), '; my valid data count:', len(valid_classify_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 2: 模型探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InceptionResNetV2探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "624/624 [==============================] - 714s 1s/step - loss: 0.7868 - acc: 0.5256 - val_loss: 0.7285 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72851, saving model to saved_models/weights.best.InceptionResNetV2.hdf5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a1c1fa8f3c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    442\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0msymbolic_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(ops)\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \"\"\"\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构建不带分类器的预训练模型\n",
    "x = Input(shape=(299, 299, 3))\n",
    "x = Lambda(inception_resnet_v2.preprocess_input)(x)\n",
    "base_model = InceptionResNetV2(input_tensor=x, weights='imagenet', include_top=False)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# 添加全局平均池化层\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "# 添加Dropout\n",
    "x = Dropout(0.5)(x)\n",
    "# 添加一个分类器\n",
    "predictions = Dense(2, activation='sigmoid')(x)\n",
    "# 构建我们需要训练的完整模型\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# 编译模型\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(X_train, y_train, batch_size=128, nb_epoch=8, validation_split=0.2)\n",
    "# X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "#训练模型并保存具有最佳验证loss的模型权重\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.InceptionResNetV2.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "model.fit_generator(train_generator,\n",
    "                    epochs=5,\n",
    "                    validation_data=valid_generator,\n",
    "                    callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载具有最佳验证loss的模型权重\n",
    "model.load_weights('saved_models/weights.best.InceptionResNetV2.hdf5')\n",
    "\n",
    "y_pred = model.predict(X_test, verbose=1)\n",
    "# 将每个预测值限制到了 [0.005, 0.995] 个区间内，这个原因很简单，kaggle 官方的评估标准是 LogLoss，\n",
    "# 对于预测正确的样本，0.995 和 1 相差无几，但是对于预测错误的样本，0 和 0.005 的差距非常大，是 15 和 2 的差别。\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html\n",
    "y_pred = y_pred.clip(min=0.005, max=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-51c129c7d5bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sample_submission.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred.csv', index=None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InceptionV3探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19968 images belonging to 2 classes.\n",
      "Found 5050 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:44: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:44: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., callbacks=[<keras.ca..., steps_per_epoch=64, epochs=10, validation_steps=1024)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 414s 6s/step - loss: 0.4123 - acc: 0.8315 - val_loss: 1.5902 - val_acc: 0.5005\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59020, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 405s 6s/step - loss: 0.2222 - acc: 0.9316 - val_loss: 2.2739 - val_acc: 0.5001\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.59020\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 405s 6s/step - loss: 0.1892 - acc: 0.9370 - val_loss: 2.5419 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.59020\n",
      "Epoch 4/10\n",
      "63/64 [============================>.] - ETA: 0s - loss: 0.1553 - acc: 0.9494"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d90c6890dd25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    209\u001b[0m                             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                             max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m    212\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1478\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    323\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# gen = ImageDataGenerator()\n",
    "# 这里要设置class_mode=\"binary\"(二分类问题), 如果设置class_mode=\"categorical\", 那么下面Dense神经元设置1个会报错\n",
    "# gen = ImageDataGenerator(preprocessing_function = inception_v3.preprocess_input)\n",
    "gen = ImageDataGenerator(rescale=1.0/255)\n",
    "train_generator = gen.flow_from_directory(directory=\"train_classify\", \n",
    "                                            target_size=(299, 299), \n",
    "                                            batch_size=32,\n",
    "                                            class_mode=\"binary\")\n",
    "\n",
    "valid_generator = gen.flow_from_directory(directory='valid_classify',\n",
    "                                               target_size=(299, 299),\n",
    "                                               batch_size=32,\n",
    "                                               class_mode='binary')\n",
    "x = Input(shape=(299, 299, 3))\n",
    "# x = inception_v3.preprocess_input(x)\n",
    "\n",
    "base_model = InceptionV3(input_tensor=x, weights='imagenet', include_top=False)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 添加全局平均池化层\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "# 添加Dropout\n",
    "x = Dropout(0.5)(x)\n",
    "# 添加一个分类器\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.InceptionV3.hdf5',\n",
    "                               verbose=1, save_best_only=True)\n",
    "# model.fit_generator(train_generator,\n",
    "#                     epochs=5,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     callbacks=[checkpointer])\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=2048,\n",
    "        nb_epoch=10,\n",
    "        validation_data=valid_generator,\n",
    "        nb_val_samples=1024,\n",
    "        callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xception探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 3: 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "上面单独使用了Resnet50, InceptionV3, Exception模型进行尝试，成绩都不是太理想；\n",
    "下面会综合三个模型的特征来搭建一个全新的模型进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 4: 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 5: 模型调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 6: 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 步骤 7: 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
